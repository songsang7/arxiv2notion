import os
import json
import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
from google import genai
import time
from google.genai import types
import httpx

NOTION_TOKEN = os.environ.get("NOTION_TOKEN")
DATABASE_ID = os.environ.get("DATABASE_ID")
GOOGLE_API_KEY = os.environ.get("GOOGLE_API_KEY")

# âœ… Hard-code non-secret configurations directly in the script
KEYWORDS = [
    "audio language model",
    "speech language model",
    "speech style",
    "spoken language model",
    "speech to speech",
    "audio to speech",
    "Omni",
    "voice assistant"
  ]
ALLOWED_SUBJECTS = {"cs.CL", "cs.AI", "cs.LG"}
MY_RESEARCH_AREA = "My research focuses on developing virtual agents that understand user situations by jointly reasoning over user speech and ambient sounds as multimodal input, with a particular emphasis on generating speech with diverse styles using audio language models."
LOOKBACK_DAYS = 3

# Basic check to ensure secrets were loaded
if not all([NOTION_TOKEN, DATABASE_ID, GOOGLE_API_KEY]):
    raise ValueError("âŒ One or more secret environment variables are not set. Please check your GitHub repository secrets.")

MODEL_LIST = ["gemini-2.5-pro", "gemini-2.5-flash", "gemini-2.0-flash", "gemini-2.5-flash-lite-preview-06-17"]

current_model_index = 0 # ì‚¬ìš©í•  ëª¨ë¸ì„ ê°€ë¦¬í‚¤ëŠ” ì¸ë±ìŠ¤

# âœ… ë‚ ì§œ ê³„ì‚°ë„ config ê¸°ë°˜ìœ¼ë¡œ
today = datetime.today()
yesterday = today - timedelta(days=LOOKBACK_DAYS)

# âœ… Gemini client ì„¤ì •
client = genai.Client(api_key=GOOGLE_API_KEY)

def fetch_existing_titles():
    """Notion ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê¸°ì¡´ ë…¼ë¬¸ ì œëª©ë“¤ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    url = f"https://api.notion.com/v1/databases/{DATABASE_ID}/query"
    headers = {
        "Authorization": f"Bearer {NOTION_TOKEN}",
        "Notion-Version": "2022-06-28",
        "Content-Type": "application/json"
    }
    titles = set()
    has_more = True
    next_cursor = None
    while has_more:
        data = {"start_cursor": next_cursor} if next_cursor else {}
        try:
            res = requests.post(url, headers=headers, json=data, timeout=10)
            res.raise_for_status()
            results = res.json()
            for page in results["results"]:
                try:
                    # âœ¨ ê³µë°± ì •ê·œí™” ì¶”ê°€
                    title = ' '.join(page["properties"]["Paper"]["title"][0]["text"]["content"].split())
                    titles.add(title)
                except (KeyError, IndexError):
                    continue
            has_more = results.get("has_more", False)
            next_cursor = results.get("next_cursor")
        except requests.exceptions.RequestException as e:
            print(f"âŒ Notion ì œëª© ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            break
    return titles

def fetch_arxiv_papers():
    """í‚¤ì›Œë“œë¥¼ ê¸°ë°˜ìœ¼ë¡œ arXivì—ì„œ ë…¼ë¬¸ì„ ê²€ìƒ‰í•˜ê³  ë‚ ì§œì™€ ì¹´í…Œê³ ë¦¬ë¡œ í•„í„°ë§í•©ë‹ˆë‹¤."""
    base_url = "http://export.arxiv.org/api/query?"
    unique_papers = {}
    print("â¬‡ï¸  í‚¤ì›Œë“œ ê¸°ë°˜ arXiv ë…¼ë¬¸ ë‹¤ìš´ë¡œë“œ ì‹œì‘...")
    for keyword in set(KEYWORDS):
        print(f"ğŸ” í‚¤ì›Œë“œ ê²€ìƒ‰ ì¤‘: \"{keyword}\"")
        search_query = f'ti:"{keyword}" OR abs:"{keyword}"'
        params = f"search_query={search_query}&sortBy=submittedDate&sortOrder=descending&max_results=50"
        try:
            response = requests.get(base_url + params, timeout=10)
            response.raise_for_status()
        except requests.exceptions.RequestException as e:
            print(f"âŒ \"{keyword}\" ê²€ìƒ‰ ì¤‘ arXiv API ì˜¤ë¥˜: {e}")
            continue
        soup = BeautifulSoup(response.content, 'xml')
        entries = soup.find_all('entry')
        for entry in entries:
            # ArXiv ID (e.g., http://arxiv.org/abs/2401.12345)
            paper_abs_url = entry.id.text.strip()
            # PDF URL (e.g., http://arxiv.org/pdf/2401.12345.pdf)
            paper_pdf_url = paper_abs_url.replace('abs', 'pdf')

            if paper_abs_url not in unique_papers:
                # âœ¨ ì œëª©ê³¼ ì´ˆë¡ì˜ ì—°ì† ê³µë°± ë° ì¤„ë°”ê¿ˆì„ í•˜ë‚˜ì˜ ê³µë°±ìœ¼ë¡œ ë³€ê²½
                clean_title = ' '.join(entry.title.text.strip().split())
                clean_abstract = ' '.join(entry.summary.text.strip().split())

                unique_papers[paper_abs_url] = {
                    'title': clean_title,
                    'link': paper_abs_url, # Abstract page URL
                    'pdf_link': paper_pdf_url, # PDF URL
                    'updated_str': entry.updated.text,
                    'abstract': clean_abstract, # âœ¨ ì›ë³¸ ì´ˆë¡ (ìš”ì•½ ì „)
                    'author': entry.author.find('name').text.strip() if entry.author else 'arXiv',
                    'categories': [cat['term'] for cat in entry.find_all('category')]
                }
        time.sleep(1)
    print(f"ğŸ‘ ì´ {len(unique_papers)}ê°œì˜ ê³ ìœ  ë…¼ë¬¸ ë°œê²¬. í•„í„°ë§ ì‹œì‘...")
    filtered_papers = []
    for paper in unique_papers.values():
        updated_date = datetime.strptime(paper['updated_str'], "%Y-%m-%dT%H:%M:%SZ").date()
        if not (yesterday.date() <= updated_date <= today.date()):
            continue
        if not any(subject in paper['categories'] for subject in ALLOWED_SUBJECTS):
            continue
        filtered_papers.append(paper)
    return filtered_papers

def analyze_paper_with_gemini(paper):
    """
    Geminië¥¼ ì‚¬ìš©í•˜ì—¬ PDF ë…¼ë¬¸ì„ ë¶„ì„í•˜ê³ , í•œêµ­ì–´ ìš”ì•½ê³¼ ê´€ë ¨ë„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    API ì¿¼í„° ì†Œì§„ ì‹œ, ìë™ìœ¼ë¡œ ë‹¤ìŒ ëª¨ë¸ë¡œ ì „í™˜í•˜ì—¬ ì¬ì‹œë„í•©ë‹ˆë‹¤.
    """
    global current_model_index

    # --- PDF ë‹¤ìš´ë¡œë“œ ---
    pdf_url = paper['pdf_link']
    try:
        print(f"    - PDF ë‹¤ìš´ë¡œë“œ ì¤‘: {pdf_url}")
        doc_response = httpx.get(pdf_url, timeout=30)
        doc_response.raise_for_status()
        doc_data = doc_response.content
        print("    - PDF ë‹¤ìš´ë¡œë“œ ì™„ë£Œ.")
    except httpx.RequestError as e:
        print(f"    âŒ PDF ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None, None
    except httpx.HTTPStatusError as e:
        print(f"    âŒ PDFë¥¼ ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜ ì„œë²„ ì˜¤ë¥˜: {e}")
        return None, None

    # --- Gemini í”„ë¡¬í”„íŠ¸ ---
    prompt = f"""
    ë‹¹ì‹ ì€ ì—°êµ¬ì›ì„ ë•ëŠ” AI ì¡°ìˆ˜ì…ë‹ˆë‹¤. ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” ì²¨ë¶€ëœ PDF ë…¼ë¬¸ì„ ë¶„ì„í•˜ì—¬ ë‘ ê°€ì§€ ê²°ê³¼ë¬¼ì„ ì œê³µí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤: í•œêµ­ì–´ ìš”ì•½, ê·¸ë¦¬ê³  ë‚˜ì˜ ì—°êµ¬ ë¶„ì•¼ì™€ì˜ ê´€ë ¨ì„± íŒë‹¨.

    **ë‚˜ì˜ ì—°êµ¬ ë¶„ì•¼:**
    "{MY_RESEARCH_AREA}"

    **ì§€ì‹œì‚¬í•­:**
    1.  **ë…¼ë¬¸ ìš”ì•½ (í•œêµ­ì–´):** ë…¼ë¬¸ì˜ í•µì‹¬ ë‚´ìš©ì„ í•œêµ­ì–´ë¡œ ìš”ì•½í•´ ì£¼ì„¸ìš”. ìš”ì•½ì—ëŠ” ë‹¤ìŒ ë‚´ìš©ì´ ë°˜ë“œì‹œ í¬í•¨ë˜ì–´ì•¼ í•©ë‹ˆë‹¤:
        * **Motivation:** ì´ ì—°êµ¬ê°€ í•´ê²°í•˜ê³ ì í•˜ëŠ” ë¬¸ì œëŠ” ë¬´ì—‡ì´ë©°, ì™œ ì¤‘ìš”í•œê°€?
        * **Proposed Method:** ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì €ìë“¤ì´ ì œì•ˆí•˜ëŠ” ìƒˆë¡œìš´ ë°©ë²•ë¡ ì´ë‚˜ ì ‘ê·¼ ë°©ì‹ì€ ë¬´ì—‡ì¸ê°€? ê¸°ì¡´ ë°©ë²•ë“¤ê³¼ì˜ ì°¨ì´ì ì€ ë¬´ì—‡ì¸ê°€?
        * **Results:** ì œì•ˆëœ ë°©ë²•ì˜ íš¨ê³¼ë¥¼ ë³´ì—¬ì£¼ëŠ” ì£¼ìš” ê²°ê³¼ëŠ” ë¬´ì—‡ì¸ê°€?
        * **ì‘ì„± ìŠ¤íƒ€ì¼:** ë¶ˆí•„ìš”í•œ ì´ëª¨í‹°ì½˜ì´ë‚˜ íŠ¹ìˆ˜ë¬¸ì ì—†ì´, ì™„ì „í•œ ë¬¸ì¥ìœ¼ë¡œ êµ¬ì„±ëœ ì¤„ê¸€ í˜•íƒœë¡œ ì‘ì„±í•´ ì£¼ì„¸ìš”.

    2.  **ê´€ë ¨ì„± íŒë‹¨:** ë…¼ë¬¸ì˜ ê¸°ì—¬ê°€ ë‚˜ì˜ ì—°êµ¬ ë¶„ì•¼ì— ì§ì ‘ì ìœ¼ë¡œ ê´€ë ¨ì´ ìˆëŠ”ì§€ í‰ê°€í•´ ì£¼ì„¸ìš”.

    3.  **ì¶œë ¥ í˜•ì‹:** ë°˜ë“œì‹œ ì•„ë˜ í˜•ì‹ì„ ì •í™•íˆ ì§€ì¼œì„œ ì‘ë‹µí•´ì•¼ í•˜ë©°, "|||"ë¥¼ êµ¬ë¶„ìë¡œ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤. ë‹¤ë¥¸ ì¶”ê°€ì ì¸ ì„¤ëª…ì´ë‚˜ ì¸ì‚¬ë§ì„ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.

    **ì¶œë ¥ í˜•ì‹:**
    [ì—¬ê¸°ì— í•œêµ­ì–´ ìš”ì•½ì„ ì‘ì„±í•˜ì„¸ìš”.]|||[Yes. ë˜ëŠ” No.]
    """

    while current_model_index < len(MODEL_LIST):
        model_to_use = MODEL_LIST[current_model_index]
        print(f"    - Gemini ë¶„ì„ ì‹œë„ (ëª¨ë¸: {model_to_use})")

        try:
            # API í˜¸ì¶œ (PDF ë°ì´í„°ì™€ í”„ë¡¬í”„íŠ¸ë¥¼ í•¨ê»˜ ì „ì†¡)
            response = client.models.generate_content(
                model = model_to_use,
                contents=[
                    types.Part.from_data(
                        data=doc_data,
                        mime_type='application/pdf',
                    ),
                    prompt
                ]
            )

            # ì‘ë‹µ ì²˜ë¦¬
            if response.text and '|||' in response.text:
                parts = response.text.strip().split('|||')
                if len(parts) == 2:
                    summary = parts[0].strip()
                    answer_part = parts[1].strip().lower()
                    if "yes" in answer_part:
                        return "Related", summary
                    elif "no" in answer_part:
                        return "Unrelated", summary

            print(f"    âš ï¸ Geminiê°€ ì˜ˆìƒì¹˜ ëª»í•œ í˜•ì‹ìœ¼ë¡œ ë‹µë³€: {response.text}...")
            return None, None

        except Exception as e:
            error_message = str(e).lower()
            if "resource_exhausted" in error_message or "quota" in error_message:
                print(f"    âš ï¸ ëª¨ë¸ '{model_to_use}'ì˜ API ì¿¼í„° ì†Œì§„. ë‹¤ìŒ ëª¨ë¸ë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
                current_model_index += 1
                time.sleep(2)
                continue
            else:
                print(f"    âŒ Gemini API í˜¸ì¶œ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
                return None, None

    print("    âŒ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“  Gemini ëª¨ë¸ì˜ ì¿¼í„°ë¥¼ ì†Œì§„í–ˆìŠµë‹ˆë‹¤. ë¶„ì„ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
    return None, None

# âœ… Notionì— ë…¼ë¬¸ ì¶”ê°€ (ë³€ê²½ ì—†ìŒ - ì´ë¯¸ ìš”ì•½ë³¸ì„ ë°›ë„ë¡ ì„¤ê³„ë¨)
def add_to_notion(paper, related_status):
    """ë…¼ë¬¸ ì •ë³´, ê´€ë ¨ë„ ìƒíƒœ, ë°œí–‰ì¼ì„ Notionì— ì¶”ê°€í•©ë‹ˆë‹¤."""
    url = "https://api.notion.com/v1/pages"
    headers = {
        "Authorization": f"Bearer {NOTION_TOKEN}",
        "Content-Type": "application/json",
        "Notion-Version": "2022-06-28"
    }

    # âœ¨ arXivì˜ 'updated' ë‚ ì§œ(ì˜ˆ: 2025-07-02T10:00:00Z)ì—ì„œ 'YYYY-MM-DD' ë¶€ë¶„ë§Œ ì¶”ì¶œ
    updated_str = paper['updated_str'].split('T')[0]

    properties = {
        "Paper": {"title": [{"text": {"content": paper['title']}}]},
        "Abstract": {"rich_text": [{"text": {"content": paper.get('abstract', '')}}]},
        "Author": {"rich_text": [{"text": {"content": paper.get('author', 'arXiv')}}]},
        "Relatedness": {"select": {"name": related_status}},
        "url": {"url": paper['link']},
        # âœ¨ 'Date' ì†ì„±ì— ì¶”ì¶œí•œ ë‚ ì§œë¥¼ ì¶”ê°€í•˜ëŠ” ë¶€ë¶„
        "Date": {
            "date": {
                "start": updated_str
            }
        }
    }

    data = {
        "parent": {"database_id": DATABASE_ID},
        "properties": properties
    }

    try:
        res = requests.post(url, headers=headers, json=data, timeout=10)
        # ì„±ê³µ(200)ì´ë“  ì‹¤íŒ¨ë“  ì‘ë‹µ ë‚´ìš©ì„ ì¶œë ¥í•˜ë„ë¡ ìˆ˜ì •
        print(f"ğŸ“„ Notion ì‘ë‹µ: {res.status_code}")
        print(res.text) # Notionì´ ë³´ë‚´ì¤€ ìƒì„¸ ì‘ë‹µ ë‚´ìš© í™•ì¸

        if res.status_code == 200:
            print(f"âœ… Notion ë“±ë¡ ì„±ê³µ: {paper['title'][:60]}... (ìƒíƒœ: {related_status})")
        else:
            print(f"âŒ Notion ë“±ë¡ ì‹¤íŒ¨: {paper['title'][:60]}...")
    except requests.exceptions.RequestException as e:
        print(f"âŒ Notion API ìš”ì²­ ì‹¤íŒ¨: {paper['title'][:60]}... | {e}")


def main():
    """ë©”ì¸ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ ë…¼ë¬¸ ìë™í™” ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")

    # 1. Notion DBì—ì„œ ê¸°ì¡´ ë…¼ë¬¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    print("\n[1/4] ğŸ“š Notion DBì—ì„œ ê¸°ì¡´ ë…¼ë¬¸ ëª©ë¡ ê°€ì ¸ì˜¤ëŠ” ì¤‘...")
    existing_titles = fetch_existing_titles()
    print(f"ì´ {len(existing_titles)}ê°œì˜ ë…¼ë¬¸ì´ Notionì— ì¡´ì¬í•©ë‹ˆë‹¤.")

    # 2. arXivì—ì„œ ì‹ ê·œ ë…¼ë¬¸ ê²€ìƒ‰ ë° í•„í„°ë§
    print("\n[2/4] ğŸ” arXivì—ì„œ ì‹ ê·œ ë…¼ë¬¸ ê²€ìƒ‰ ë° í•„í„°ë§ ì¤‘...")
    arxiv_papers = fetch_arxiv_papers()
    print(f"ğŸ‘ ë‚ ì§œ/ì£¼ì œ í•„í„° í†µê³¼í•œ ë…¼ë¬¸ ìˆ˜: {len(arxiv_papers)}")

    # 3. Gemini í•„í„°ë§ ë° ìµœì¢… ì¤‘ë³µ ê²€ì‚¬
    final_papers_to_add = []
    if arxiv_papers:
        print("\n[3/4] ğŸ¤– Gemini ê´€ë ¨ë„ ë¶„ì„ ë° ì´ˆë¡ ìš”ì•½ ì‹œì‘...")
        new_papers = []
        for paper in arxiv_papers:
            # âœ¨ ê³µë°± ì •ê·œí™”ëœ ì œëª©ìœ¼ë¡œ ì¤‘ë³µ ê²€ì‚¬
            if paper['title'] not in existing_titles:
                new_papers.append(paper)

        print(f"ì¤‘ë³µì„ ì œì™¸í•œ ì‹ ê·œ ë…¼ë¬¸ {len(new_papers)}ê°œë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.")

        for i, paper in enumerate(new_papers):
            print(f"({i+1}/{len(new_papers)}) ğŸ”¬ Gemini ë¶„ì„ ì¤‘: {paper['title'][:60]}...")
            # âœ¨ Gemini í•¨ìˆ˜ê°€ ì´ì œ 2ê°œì˜ ê°’ì„ ë°˜í™˜ (ìƒíƒœ, ìš”ì•½ë³¸)
            related_status, summarized_abstract = analyze_paper_with_gemini(paper)

            if related_status and summarized_abstract:
                # âœ¨ paper ê°ì²´ì˜ abstractë¥¼ ìš”ì•½ë³¸ìœ¼ë¡œ êµì²´
                paper['abstract'] = summarized_abstract
                final_papers_to_add.append((paper, related_status))
                print(f"ğŸ‘ Gemini ë¶„ì„ ì™„ë£Œ! (ìƒíƒœ: {related_status})")
            else:
                print(f"ğŸ‘ Gemini ë¶„ì„ ì‹¤íŒ¨. ì´ ë…¼ë¬¸ì€ ë“±ë¡ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            time.sleep(1) # Gemini API ê³¼í˜¸ì¶œ ë°©ì§€

    # 4. ìµœì¢… ëª©ë¡ì„ Notionì— ì¶”ê°€
    print(f"\n[4/4] ğŸ“ Notion DBì— ìµœì¢… ë…¼ë¬¸ ë“±ë¡ ì‹œì‘...")
    if not final_papers_to_add:
        print("âœ¨ ìƒˆë¡œ ì¶”ê°€í•  ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.")
    else:
        print(f"ì´ {len(final_papers_to_add)}ê°œì˜ ìƒˆë¡œìš´ ë…¼ë¬¸ì„ Notionì— ì¶”ê°€í•©ë‹ˆë‹¤.")
        for paper, status in final_papers_to_add:
            add_to_notion(paper, status)
            time.sleep(0.5) # Notion API ì†ë„ ì œí•œ ê³ ë ¤

    print("\nğŸ‰ ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")


if __name__ == "__main__":
    main()
