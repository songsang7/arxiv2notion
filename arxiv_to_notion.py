import os
import json
import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
from google import genai
import time
from google.genai import types
import httpx
import re

NOTION_TOKEN = os.environ.get("NOTION_TOKEN")
DATABASE_ID = os.environ.get("DATABASE_ID")
GOOGLE_API_KEY = os.environ.get("GOOGLE_API_KEY")

# âœ… Hard-code non-secret configurations directly in the script
KEYWORDS = [
    "audio language model",
    "speech language model",
    "speech style",
    "spoken language model",
    "speech to speech",
    "audio to speech",
    "Omni",
    "voice assistant"
  ]
ALLOWED_SUBJECTS = {"cs.CL", "cs.AI", "cs.LG"}
MY_RESEARCH_AREA = "My research focuses on developing virtual agents that understand user situations by jointly reasoning over user speech and ambient sounds as multimodal input, with a particular emphasis on generating speech with diverse styles using audio language models."
LOOKBACK_DAYS = 3

# Basic check to ensure secrets were loaded
if not all([NOTION_TOKEN, DATABASE_ID, GOOGLE_API_KEY]):
    raise ValueError("âŒ One or more secret environment variables are not set. Please check your GitHub repository secrets.")

MODEL_LIST = ["gemini-2.5-pro", "gemini-2.5-flash", "gemini-2.0-flash", "gemini-2.5-flash-lite-preview-06-17"]

current_model_index = 0 # ì‚¬ìš©í•  ëª¨ë¸ì„ ê°€ë¦¬í‚¤ëŠ” ì¸ë±ìŠ¤

# âœ… ë‚ ì§œ ê³„ì‚°ë„ config ê¸°ë°˜ìœ¼ë¡œ
today = datetime.today()
yesterday = today - timedelta(days=LOOKBACK_DAYS)

# âœ… Gemini client ì„¤ì •
client = genai.Client(api_key=GOOGLE_API_KEY)

def fetch_existing_titles():
    """Notion ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê¸°ì¡´ ë…¼ë¬¸ ì œëª©ë“¤ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    url = f"https://api.notion.com/v1/databases/{DATABASE_ID}/query"
    headers = {
        "Authorization": f"Bearer {NOTION_TOKEN}",
        "Notion-Version": "2022-06-28",
        "Content-Type": "application/json"
    }
    titles = set()
    has_more = True
    next_cursor = None
    while has_more:
        data = {"start_cursor": next_cursor} if next_cursor else {}
        try:
            res = requests.post(url, headers=headers, json=data, timeout=10)
            res.raise_for_status()
            results = res.json()
            for page in results["results"]:
                try:
                    # âœ¨ ê³µë°± ì •ê·œí™” ì¶”ê°€
                    title = ' '.join(page["properties"]["Paper"]["title"][0]["text"]["content"].split())
                    titles.add(title)
                except (KeyError, IndexError):
                    continue
            has_more = results.get("has_more", False)
            next_cursor = results.get("next_cursor")
        except requests.exceptions.RequestException as e:
            print(f"âŒ Notion ì œëª© ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            break
    return titles

def fetch_arxiv_papers():
    """í‚¤ì›Œë“œë¥¼ ê¸°ë°˜ìœ¼ë¡œ arXivì—ì„œ ë…¼ë¬¸ì„ ê²€ìƒ‰í•˜ê³  ë‚ ì§œì™€ ì¹´í…Œê³ ë¦¬ë¡œ í•„í„°ë§í•©ë‹ˆë‹¤."""
    base_url = "http://export.arxiv.org/api/query?"
    unique_papers = {}
    print("â¬‡ï¸  í‚¤ì›Œë“œ ê¸°ë°˜ arXiv ë…¼ë¬¸ ë‹¤ìš´ë¡œë“œ ì‹œì‘...")
    for keyword in set(KEYWORDS):
        print(f"ğŸ” í‚¤ì›Œë“œ ê²€ìƒ‰ ì¤‘: \"{keyword}\"")
        search_query = f'ti:"{keyword}" OR abs:"{keyword}"'
        params = f"search_query={search_query}&sortBy=submittedDate&sortOrder=descending&max_results=50"
        try:
            response = requests.get(base_url + params, timeout=10)
            response.raise_for_status()
        except requests.exceptions.RequestException as e:
            print(f"âŒ \"{keyword}\" ê²€ìƒ‰ ì¤‘ arXiv API ì˜¤ë¥˜: {e}")
            continue
        soup = BeautifulSoup(response.content, 'xml')
        entries = soup.find_all('entry')
        for entry in entries:
            # ArXiv ID (e.g., http://arxiv.org/abs/2401.12345)
            paper_abs_url = entry.id.text.strip()
            # PDF URL (e.g., http://arxiv.org/pdf/2401.12345.pdf)
            paper_pdf_url = paper_abs_url.replace('abs', 'pdf')

            if paper_abs_url not in unique_papers:
                # âœ¨ ì œëª©ê³¼ ì´ˆë¡ì˜ ì—°ì† ê³µë°± ë° ì¤„ë°”ê¿ˆì„ í•˜ë‚˜ì˜ ê³µë°±ìœ¼ë¡œ ë³€ê²½
                clean_title = ' '.join(entry.title.text.strip().split())
                clean_abstract = ' '.join(entry.summary.text.strip().split())

                unique_papers[paper_abs_url] = {
                    'title': clean_title,
                    'link': paper_abs_url, # Abstract page URL
                    'pdf_link': paper_pdf_url, # PDF URL
                    'updated_str': entry.updated.text,
                    'abstract': clean_abstract, # âœ¨ ì›ë³¸ ì´ˆë¡ (ìš”ì•½ ì „)
                    'author': entry.author.find('name').text.strip() if entry.author else 'arXiv',
                    'categories': [cat['term'] for cat in entry.find_all('category')]
                }
        time.sleep(1)
    print(f"ğŸ‘ ì´ {len(unique_papers)}ê°œì˜ ê³ ìœ  ë…¼ë¬¸ ë°œê²¬. í•„í„°ë§ ì‹œì‘...")
    filtered_papers = []
    for paper in unique_papers.values():
        updated_date = datetime.strptime(paper['updated_str'], "%Y-%m-%dT%H:%M:%SZ").date()
        if not (yesterday.date() <= updated_date <= today.date()):
            continue
        if not any(subject in paper['categories'] for subject in ALLOWED_SUBJECTS):
            continue
        filtered_papers.append(paper)
    return filtered_papers

def analyze_paper_with_gemini(paper):
    """
    Geminië¥¼ ì‚¬ìš©í•˜ì—¬ PDF ë…¼ë¬¸ì„ ë¶„ì„í•˜ê³ , ìš”ì•½ì„ 5ê°œ í•­ëª©ìœ¼ë¡œ íŒŒì‹±í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    global current_model_index

    # --- PDF ë‹¤ìš´ë¡œë“œ ---
    try:
        print(f"  - PDF ë‹¤ìš´ë¡œë“œ ì¤‘: {paper['pdf_link']}")
        doc_response = httpx.get(paper['pdf_link'], timeout=30)
        doc_response.raise_for_status()
        doc_data = doc_response.content
        print("  - PDF ë‹¤ìš´ë¡œë“œ ì™„ë£Œ.")
    except (httpx.RequestError, httpx.HTTPStatusError) as e:
        print(f"  âŒ PDF ë‹¤ìš´ë¡œë“œ/ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return None, None

    # --- Gemini í”„ë¡¬í”„íŠ¸ (í•­ëª©ë³„ íƒœê·¸ ì¶”ê°€) ---
    prompt = f"""
    You are an AI assistant helping a researcher. Your task is to analyze the attached PDF paper and provide two outputs: an English summary divided into five specific sections, and an assessment of its relevance.

    **My Research Area:**
    "{MY_RESEARCH_AREA}"

    **Instructions:**

    1.  **Paper Summary (English):** Please summarize the paper, strictly following the five-part structure below. Use the exact tags `[MOTIVATION]`, `[DIFFERENCES]`, `[CONTRIBUTIONS]`, `[METHOD]`, `[RESULTS]` to label each section. Each section should be a concise paragraph.
        * `[MOTIVATION]`: What problem does this research aim to solve, and why is it important?
        * `[DIFFERENCES]`: How is this work different from or improving upon previous approaches?
        * `[CONTRIBUTIONS]`: What are the main contributions and novel aspects of this paper?
        * `[METHOD]`: What method or approach do the authors propose?
        * `[RESULTS]`: What are the key results that demonstrate the effectiveness of the proposed method?

    2.  **Relevance Assessment:** Please determine if the paperâ€™s contributions are directly relevant to my research area.

    3.  **Output Format:** You **MUST** follow the exact format below, using "|||" as a delimiter. Do not include any additional commentary or greetings.

    **Output Format:**
    [MOTIVATION]
    ... summary ...
    [DIFFERENCES]
    ... summary ...
    [CONTRIBUTIONS]
    ... summary ...
    [METHOD]
    ... summary ...
    [RESULTS]
    ... summary ...
    |||[Yes. or No.]
    """

    while current_model_index < len(MODEL_LIST):
        model_to_use = MODEL_LIST[current_model_index]
        print(f"  - Gemini ë¶„ì„ ì‹œë„ (ëª¨ë¸: {model_to_use})")
        

        try:
            response = client.models.generate_content(
                model=model_to_use,
                contents=[
                    types.Part.from_bytes(data=doc_data, mime_type='application/pdf'),
                    prompt
                ],
            )

            if response.text and '|||' in response.text:
                summary_part, answer_part = [p.strip() for p in response.text.strip().split('|||', 1)]
                
                # --- ì •ê·œí‘œí˜„ì‹ì„ ì´ìš©í•œ íŒŒì‹± ---
                tags = ["MOTIVATION", "DIFFERENCES", "CONTRIBUTIONS", "METHOD", "RESULTS"]
                parsed_summary = {}
                for i in range(len(tags)):
                    current_tag = tags[i]
                    next_tag = tags[i+1] if i + 1 < len(tags) else None
                    
                    pattern = f"\[{current_tag}\](.*?)"
                    if next_tag:
                        pattern += f"(?=\[{next_tag}\])"
                    
                    match = re.search(pattern, summary_part, re.DOTALL | re.IGNORECASE)
                    
                    if match:
                        # íŒŒì‹±ëœ ë‚´ìš©ì˜ ê¸¸ì´ê°€ 2000ìë¥¼ ë„˜ìœ¼ë©´ ì˜ë¼ë‚´ê¸°
                        content = match.group(1).strip()
                        parsed_summary[current_tag] = content[:1990] + '...' if len(content) > 2000 else content
                    else:
                        parsed_summary[current_tag] = "N/A" # í•´ë‹¹ ì„¹ì…˜ì„ ì°¾ì§€ ëª»í•œ ê²½ìš°

                # ëª¨ë“  íƒœê·¸ê°€ íŒŒì‹±ë˜ì—ˆëŠ”ì§€ í™•ì¸
                if all(tag in parsed_summary for tag in tags):
                    relevance = "Related" if "yes" in answer_part.lower() else "Unrelated"
                    return relevance, parsed_summary
            
            print(f"  âš ï¸ Geminiê°€ ì˜ˆìƒì¹˜ ëª»í•œ í˜•ì‹ìœ¼ë¡œ ë‹µë³€: {response.text[:200]}...")
            return None, None

        except Exception as e:
            if "overload" in str(e).lower():
                time.sleep(30)
            else:
                if "resource_exhausted" in str(e).lower() or "quota" in str(e).lower():
                    print(f"  âš ï¸ ëª¨ë¸ '{model_to_use}'ì˜ API ì¿¼í„° ì†Œì§„. ë‹¤ìŒ ëª¨ë¸ë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
                    current_model_index += 1
                    time.sleep(2)
                else:
                    print(f"  âŒ Gemini API í˜¸ì¶œ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
                    return None, None

    print("  âŒ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“  Gemini ëª¨ë¸ì˜ ì¿¼í„°ë¥¼ ì†Œì§„í–ˆìŠµë‹ˆë‹¤.")
    return None, None

# âœ… Notionì— ë…¼ë¬¸ ì¶”ê°€ (ë³€ê²½ ì—†ìŒ - ì´ë¯¸ ìš”ì•½ë³¸ì„ ë°›ë„ë¡ ì„¤ê³„ë¨)
def add_to_notion(paper, related_status, summary_parts):
    """ë…¼ë¬¸ ì •ë³´, ê´€ë ¨ë„, ë¶„í• ëœ ìš”ì•½ì„ Notionì— ì¶”ê°€í•©ë‹ˆë‹¤."""
    url = "https://api.notion.com/v1/pages"
    headers = {
        "Authorization": f"Bearer {NOTION_TOKEN}",
        "Content-Type": "application/json",
        "Notion-Version": "2022-06-28"
    }

    updated_str = paper['updated_str'].split('T')[0]

    # Notion ì†ì„± ì´ë¦„ê³¼ summary_partsì˜ í‚¤ë¥¼ ì •í™•íˆ ì¼ì¹˜ì‹œì¼œì•¼ í•©ë‹ˆë‹¤.
    # ì˜ˆ: Notion ì†ì„± ì´ë¦„ 'Motivation' -> summary_parts['MOTIVATION']
    properties = {
        "Paper": {"title": [{"text": {"content": paper['title']}}]},
        "Abstract": {"rich_text": [{"text": {"content": paper.get('abstract', '')}}]}, # ì›ë³¸ ì´ˆë¡ ì €ì¥
        "Author": {"rich_text": [{"text": {"content": paper.get('author', 'arXiv')}}]},
        "Relatedness": {"select": {"name": related_status}},
        "url": {"url": paper['link']},
        "Date": {"date": {"start": updated_str}},
        # --- ë¶„í• ëœ ìš”ì•½ ì¶”ê°€ ---
        "Motivation": {"rich_text": [{"text": {"content": summary_parts.get('MOTIVATION', 'N/A')}}]},
        "Differences from Prior Work": {"rich_text": [{"text": {"content": summary_parts.get('DIFFERENCES', 'N/A')}}]},
        "Contributions and Novelty": {"rich_text": [{"text": {"content": summary_parts.get('CONTRIBUTIONS', 'N/A')}}]},
        "Proposed Method": {"rich_text": [{"text": {"content": summary_parts.get('METHOD', 'N/A')}}]},
        "Results": {"rich_text": [{"text": {"content": summary_parts.get('RESULTS', 'N/A')}}]}
    }

    data = {"parent": {"database_id": DATABASE_ID}, "properties": properties}

    try:
        res = requests.post(url, headers=headers, json=data, timeout=15)
        if res.status_code == 200:
            print(f"âœ… Notion ë“±ë¡ ì„±ê³µ: {paper['title'][:60]}... (ìƒíƒœ: {related_status})")
        else:
            print(f"âŒ Notion ë“±ë¡ ì‹¤íŒ¨: {paper['title'][:60]}...")
            print(f"ğŸ“„ Notion ì‘ë‹µ: {res.status_code}")
            print(res.text) # ì‹¤íŒ¨ ì‹œ ì—ëŸ¬ ë©”ì‹œì§€ í™•ì¸
    except requests.exceptions.RequestException as e:
        print(f"âŒ Notion API ìš”ì²­ ì‹¤íŒ¨: {paper['title'][:60]}... | {e}")


def main():
    """ë©”ì¸ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ ë…¼ë¬¸ ìë™í™” ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")

    print("\n[1/4] ğŸ“š Notion DBì—ì„œ ê¸°ì¡´ ë…¼ë¬¸ ëª©ë¡ ê°€ì ¸ì˜¤ëŠ” ì¤‘...")
    existing_titles = fetch_existing_titles()
    print(f"ì´ {len(existing_titles)}ê°œì˜ ë…¼ë¬¸ì´ Notionì— ì¡´ì¬í•©ë‹ˆë‹¤.")

    print("\n[2/4] ğŸ” arXivì—ì„œ ì‹ ê·œ ë…¼ë¬¸ ê²€ìƒ‰ ë° í•„í„°ë§ ì¤‘...")
    arxiv_papers = fetch_arxiv_papers()
    print(f"ğŸ‘ ë‚ ì§œ/ì£¼ì œ í•„í„° í†µê³¼í•œ ë…¼ë¬¸ ìˆ˜: {len(arxiv_papers)}")

    final_papers_to_add = []
    if arxiv_papers:
        print("\n[3/4] ğŸ¤– Gemini ê´€ë ¨ë„ ë¶„ì„ ë° í•­ëª©ë³„ ìš”ì•½ ì‹œì‘...")
        new_papers = [p for p in arxiv_papers if p['title'] not in existing_titles]
        print(f"ì¤‘ë³µì„ ì œì™¸í•œ ì‹ ê·œ ë…¼ë¬¸ {len(new_papers)}ê°œë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.")

        for i, paper in enumerate(new_papers):
            print(f"({i+1}/{len(new_papers)}) ğŸ”¬ Gemini ë¶„ì„ ì¤‘: {paper['title'][:60]}...")
            
            # Gemini í•¨ìˆ˜ê°€ (ìƒíƒœ, ìš”ì•½ ë”•ì…”ë„ˆë¦¬)ë¥¼ ë°˜í™˜
            related_status, summary_parts = analyze_paper_with_gemini(paper)

            if related_status and summary_parts:
                # `paper` ê°ì²´, `status`, `summary_parts` ë”•ì…”ë„ˆë¦¬ë¥¼ í•¨ê»˜ ì €ì¥
                final_papers_to_add.append((paper, related_status, summary_parts))
                print(f"ğŸ‘ Gemini ë¶„ì„ ì™„ë£Œ! (ìƒíƒœ: {related_status})")
            else:
                print(f"ğŸ‘ Gemini ë¶„ì„ ì‹¤íŒ¨. ì´ ë…¼ë¬¸ì€ ë“±ë¡ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            time.sleep(1)

    print(f"\n[4/4] ğŸ“ Notion DBì— ìµœì¢… ë…¼ë¬¸ ë“±ë¡ ì‹œì‘...")
    if not final_papers_to_add:
        print("âœ¨ ìƒˆë¡œ ì¶”ê°€í•  ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.")
    else:
        print(f"ì´ {len(final_papers_to_add)}ê°œì˜ ìƒˆë¡œìš´ ë…¼ë¬¸ì„ Notionì— ì¶”ê°€í•©ë‹ˆë‹¤.")
        # `paper`, `status`, `parts`ë¥¼ ì˜¬ë°”ë¥´ê²Œ ì „ë‹¬
        for paper, status, parts in final_papers_to_add:
            add_to_notion(paper, status, parts)
            time.sleep(0.5)

    print("\nğŸ‰ ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")

if __name__ == "__main__":
    main()
